inferenceService:
  storage:
    storageUri: 'oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct'
    mode: uri
  args:
    - "--max-model-len=23000"
    - "--gpu-memory-utilization=0.99"
    - "--tool-call-parser llama3_json"
    - "--enable-auto-tool-choice"
  resources:
    limits:
      cpu: '4'
      memory: 20Gi
      nvidia.com/gpu: '1'
    requests:
      cpu: '2'
      memory: 16Gi
      nvidia.com/gpu: '1'
  tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Equal
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Equal
    value: "True"
endpoint:
  externalRoute:
    enabled: true
  # auth:
  #   enabled: true
  #   serviceAccounts:
  #     - name: llama-3b-service-account